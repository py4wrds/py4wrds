{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Python data science modules\n",
    "\n",
    "Now we've got a grasp on the fundamentals of Python, we're ready to use Python for data science.\n",
    "\n",
    "Unlike some more math-oriented programming languages (like R or MATLAB), Python relies on external packages to provide most data science functionality.\n",
    "\n",
    "The Python community has largely standardized on the following packages, all of which we'll be covering in this module, and will be using extensively for the rest of the course:\n",
    "\n",
    "* numpy - math tools, plus working with 1D & 2D arrays\n",
    "* pandas - working with tabular data\n",
    "* matplotlib - making plots and visualizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Numpy\n",
    "\n",
    "Numpy is a Python package for representing **array data**, and comes with a large library of tools and mathematical functions that operate efficiently on arrays, mostly through its `ndarray` class.\n",
    "\n",
    "Numpy is by far the most popular Python package for data science, and is one of the [most-downloaded](https://pypistats.org/top) python packages overall. It's so useful and reliable, that most of the mathematical functionality of the other packages covered in this module (pandas, seaborn, matplotlib) is provided by numpy under the hood.\n",
    "\n",
    "Because numpy is used a lot, it's convention to import it with the `np` abbreviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Why numpy?\n",
    "\n",
    "A numpy array is similar to a Python list: they can both serve as containers for numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_list = [0, 2, 4, 6]\n",
    "print(python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = np.array([0, 2, 4, 6])\n",
    "print(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why use numpy instead of lists?\n",
    "\n",
    "* Speed\n",
    "    * Although numpy is a Python package, most of the functionality is written in fast C or Fortran code.\n",
    "* Memory efficient\n",
    "    * Numpy uses less memory to store numbers than Python, so you can work on larger datasets.\n",
    "* Functionality\n",
    "    * Numpy comes with a huge range of modules with fast and thoroughly-validated algorithms from interpolation to fourier transforms.\n",
    "* Manipulation syntax\n",
    "    * Numpy's syntax makes it clear and easy to perform common array operations, like slicing, filtering, and summarization.\n",
    "\n",
    "\n",
    "But there are some usecases where lists make more sense\n",
    "\n",
    "* Storing different kinds of data together\n",
    "    * Numpy arrays are homogeneous, all the elements must be the same type\n",
    "* Working with non-numerical data\n",
    "    * Only some numpy functionality works with strings and other types\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating arrays\n",
    "\n",
    "One way to create an array is from a Python sequence like a list using the `array()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month_list = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "days_per_month = np.array(days_per_month_list)\n",
    "days_per_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesting lists will create higher dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "array_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As well as converting Python lists to arrays, numpy can create its own arrays!\n",
    "\n",
    "You can create an array that's filled with a certain number of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or ones (we specify the number of rows, then the number of columns for a 2D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has it's own version of the Python `range()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(2, 9, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a related `linspace()` function to create an array with evenly-spaced elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0, 10, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a whole `random` module that can create arrays from randomly sampling various distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(low=0, high=10, size=(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size and shape\n",
    "\n",
    "We can use `len()` to get the length of a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(days_per_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the `.shape` attribute will give the length of each dimension (remember the 2D order? **rows then cols**!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array arithmetic\n",
    "\n",
    "Arrays let you express mathematical equations without for loops. This makes your code much faster, plus numpy code tends to read more like a math formula than programming.\n",
    "\n",
    "Operations with single numbers are applied to the whole array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_per_month = days_per_month / 7\n",
    "print(weeks_per_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whereas operations between arrays of the same size are applied element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month + days_per_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing arrays results in an array of the same size with True/False values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_long_month = days_per_month >= 31\n",
    "print(is_long_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform boolean logic, numpy uses `&` instead of `and`, and `|` instead of `or`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_q1_month = np.arange(12) < 3\n",
    "is_long_q1_month = is_q1_month & is_long_month\n",
    "\n",
    "print(is_long_q1_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Array slicing and indexing\n",
    "\n",
    "\n",
    "Like a Python list, arrays can be sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and individual elements can be index out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(days_per_month[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You index a 2D array using the same notation: first your row slicing, then a comma `,`, then your column slicing.\n",
    "\n",
    "Here's our array as a reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example the first element of the second row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_2d[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the top three values of the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_2d[0:3, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how our column has lost its \"verticalness\": once we've sliced it out, it's just a regular 1D array.\n",
    "\n",
    "Assigning new values to an array uses the same slice syntax. A single value will be repeated to all elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d[0, :] = 99\n",
    "array_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while an equal-length sequence will be assigned elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d[:, 3] = [-1, -2, -3]\n",
    "array_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make a slice then modify it, the original array is modified too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d_slice = array_2d[1:2, 2:3]\n",
    "array_2d_slice[0] = -9999\n",
    "array_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep numpy's speedy performance, making a slice doesn't copy any data, just provides a \"view\" to the original array.\n",
    "\n",
    "If we want to modify a subset of the data independently, we can use the copy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_months = days_per_month[:3]\n",
    "q1_months_leap_year = q1_months.copy()\n",
    "q1_months_leap_year[1] = 29\n",
    "q1_months_leap_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types\n",
    "\n",
    "All values in a numpy array are converted to be the same type. By default, numpy will take an educated guess about what type we want, and we can see its choice with the `dtype` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_default = np.array([1, 2, 3])\n",
    "array_default.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can override the dtype though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_manual = np.array([1, 2, 3], dtype=np.float64)\n",
    "array_manual.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than ints and floats, the boolean dtype is frequently used in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_positive = array_2d > 0\n",
    "is_positive.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays can store strings, though of course some numpy mathematical tools won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([\"Washington\", \"Oregon\", \"California\"])\n",
    "states.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `U` in `<U10` stands for \"Unicode string\". The `<` and `10` characters represent internal numpy storage attributes ([endianess](https://stackoverflow.com/questions/24248756/python-numpy-data-io-how-to-save-data-by-different-dtype-for-each-column/24249116) and number of characters respectively).\n",
    "\n",
    "An array can also store arbitrary python objects. Lets make an array containing a number as well as the `print()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a terrible idea.\n",
    "mixed = np.array([42, print])\n",
    "mixed.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy gives up and applies the `O` for \"object\" dtype (everything in Python is an object).\n",
    "\n",
    "If you see this in the real world, it's usually a sign that you're number parsing has gone wrong! Otherwise, you may as well use a regular list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean indexing\n",
    "\n",
    "Here are some state names, with corresponding mean annual precipitation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([\"Washington\", \"Oregon\", \"California\"])\n",
    "precip = np.array([38.67, 43.62, 22.97])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pick out California we can use `==` to create a boolean array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states == \"California\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing this boolean array as indexing to another array of the same length will slice out only the elements matching `True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precip[states == \"California\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really powerful for working between corresponding arrays!\n",
    "\n",
    "Negation can be done with `!=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip[states == \"California\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by using `~` to flip all the booleans in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~(states == \"California\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = states == \"California\"\n",
    "precip[~match]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter the other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_rainfall = precip > 30\n",
    "states[high_rainfall]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical functions\n",
    "\n",
    "Numpy comes with a array versions of the functions in the builtin `math` module, and many more.\n",
    "\n",
    "Many are element-wise transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.linspace(0, 100, 4)\n",
    "\n",
    "print(nums)\n",
    "print(np.floor(nums))  # Round down to next integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while others summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a few of the more common numpy functions\n",
    "\n",
    "* Element-wise\n",
    "    * `sin`, `arcsine`, `deg2rad`: trigonometry\n",
    "    * `round`, `floor`, `ceil`: rounding\n",
    "    * `exp`, `log`, `log10`: logarithms\n",
    "    * `clip`: limit the elements in an array\n",
    "\n",
    "\n",
    "* Summarizing\n",
    "    * `mean`, `median`, `mode`: averages\n",
    "    * `std`, `var`: skew\n",
    "    * `sum`, `prod`: sum/product of all elements\n",
    "    * `percentile`, `quantile`: order\n",
    "\n",
    "\n",
    "Because numpy is so foundational to scientific Python, if you can't find a common math algorithm in numpy, there's likely to be another package that provies the function with full support for numpy arrays. \n",
    "\n",
    "Good places to hunt for extended math functions include\n",
    "\n",
    "* [scipy](https://docs.scipy.org/doc/scipy/reference/index.html) has many submodules including functions for interpolation, statistics, and linear algebra\n",
    "* [statsmodels](https://www.statsmodels.org/stable/index.html) for summary statistics\n",
    "* [pandas](https://pandas.pydata.org/) has functions for dealing with time series and strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaNs\n",
    "\n",
    "\n",
    "`NaN` stands for \"Not a Number\".\n",
    "\n",
    "\n",
    "They can come from importing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [\"1\", None, \"3\"]\n",
    "np.array(values, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as the result of mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(5) / np.arange(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but most commonly come from other NaNs!\n",
    "\n",
    "**NaNs propagate**. The result of an operation where any of the inputs is NaN, usually results in a NaN output, which can spread to your entire dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d = np.array([[np.nan, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "array_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d_normalized = array_2d / np.max(array_2d)\n",
    "array_2d_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things we can do to manage and embrace NaN values.\n",
    "\n",
    "First, if you can't discard invalid numerical data, then explicitly convert it to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate_over_time = np.array([None, 1.9, 2.1, 3.0, -1, 4.3, 4.8], dtype=np.float64)\n",
    "\n",
    "# Remove invalid data. The sensor returns -1 when broken.\n",
    "flowrate_over_time[flowrate_over_time < 0] = np.nan\n",
    "\n",
    "flowrate_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not expecting any NaNs, enforce that in the code. Numpy has some helpful functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(days_per_month)), \"NaN found!\"\n",
    "\n",
    "# isfinite will catch Inf as well as NaN, plus you don't need the negation.\n",
    "assert np.all(np.isfinite(days_per_month)), \"Invalid data found!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to ignore NaNs, many numpy functions have nan-ignoring siblings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmax(array_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll still get a NaN result if your entire input array is NaN though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(array_2d_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Pandas\n",
    "\n",
    "Pandas is a package for working with tabular spreadsheet-like data.\n",
    "\n",
    "\n",
    "### What is tabular data?\n",
    "\n",
    "Tabular data is anything in a table form! \n",
    "\n",
    "Common analytical examples include spreadsheets, CSV files, and database tables.\n",
    "\n",
    "Tabular data consists of rows and columns:\n",
    "\n",
    "* Each row represents an item, and each column represents a common feature of all the items.\n",
    "* Each row has the same columns as the other rows, in the same order.\n",
    "* A single column holds data of the same type, but different columns can have different types. \n",
    "* The order of rows sometimes matters, while the order of columns doesn't matter.\n",
    "\n",
    "\n",
    "Tabular data isn't just work spreadsheets either: for example, a music playlist is tabular data (for each song, we know the title, genre, etc) and your text message inbox is tabular data (for each conversation, we know the participants, unread status, date of most recent message, etc) \n",
    "\n",
    "Because pandas is so frequently used, it's standard to import with the `pd` abbreviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Loading data\n",
    "\n",
    "\n",
    "Pandas comes with many functions for reading lots of different kinds of data. We'll cover most of them by the end of the course, but here's a list of the main ones for your later reference\n",
    "\n",
    "* CSVs ([read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html))\n",
    "* SQL databases ([read_sql](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html))\n",
    "* Excel files ([read_excel](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html))\n",
    "* Parquet files ([read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html))\n",
    "* Any of the above, as a URL\n",
    "\n",
    "<!-- TODO: after module deployment, move most of this data loading via file to via URL (github, google sheets, and csvs/excels available on py4wrds.com) -->\n",
    "\n",
    "\n",
    "To start off, we'll load this CSV file of ground water stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"data/gwl/stations.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're loading the data directly from GitHub! This won't work for private repositories, there you'll have to download the file and use the filepath instead of the url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame inspection\n",
    "\n",
    "The code above (`df = pd.read_csv(url)`) has loaded our tabular data into an object called a DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame is one of the two core classes that pandas gives us (the other is Series which represents a column).\n",
    "\n",
    "DataFrame has a number of attributes (variables) and methods (functions) for **inspecting** our data, which is the first step in any analysis!\n",
    "\n",
    "`head()` shows us the first five rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our table has so many columns, not all are visible. We can use the `.columns` attribute to get all the names on one screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data are we working with? `.shape` gives us both the row and column count, which is always in row, col order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it's more clear to take the length directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_rows: {}\".format(len(df)))\n",
    "print(\"n_cols: {}\".format(len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dtypes` shows the data type of each column.\n",
    "\n",
    "This is important to check! Pandas makes some guesses about what data type to use, and often gets things wrong. Common pitfalls to be wary of include\n",
    "\n",
    "* Dates might be loaded as strings instead of rich datetime objects.\n",
    "* Numerical columns like `$145` or `78%` might be loaded as strings instead of as numbers\n",
    "* A single row with a typo (`32.111!`) or a non-numeric placeholder (using `Unknown` instead of `NaN`) will turn an other-wise numeric column into a string type.\n",
    "* Pandas defaults to using 64 bit integers and floats. If your dataset is maxing out your memory, you can specify 32 bit (or smaller) dtypes to reduce the size once loaded.\n",
    "* ZIP codes should be parsed as strings not integers, to avoid stripping ZIPs that begin with zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see pandas has done a pretty good job here! (the `object` type is what pandas uses to represent strings).\n",
    "\n",
    "Fixing some dtype issues can involve more complex analysis. But for simple cases, we can simply tell pandas what to do when loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, dtype={\"ZIP_CODE\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`describe()` gives a summary of our numerical columns. With the reloaded dataframe, `ZIP_CODE` is no longer considered numeric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're not always interested in the whole dataset for every analysis. You can load a subset of the columns to speed up loading, reduce memory pressure, and to just keep your workspace tidier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_county = pd.read_csv(url, usecols=[\"STN_ID\", \"WELL_DEPTH\"])\n",
    "\n",
    "df_county.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output of `df_county.tail()`, what do you think the `tail()` method does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "A column of a dataframe is a Series object. You can access a column by using its name in `[]` brackets, just like a dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"BASIN_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"COUNTY_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access columns with attribute dot notation like `df.COUNTY_NAME`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.COUNTY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's shorter and clearer! But there are some catches.\n",
    "\n",
    "* The dot notation won't work when the column name isn't a valid Python name (like if it contains spaces). \n",
    "\n",
    "* It also won't work if the column name would override a DataFrame attribute or method (like `head`).\n",
    "\n",
    "* Finally, you can't use the dot notation for assignment (the code `df.WELL_DEPTH = np.nan` won't work).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a Series to a list with `to_list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.COUNTY_NAME.to_list()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.WELL_DEPTH.to_numpy()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series supports boolean slicing with `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_san_diego = df.COUNTY_NAME == \"San Diego\"\n",
    "san_diego_ids = df.STN_ID[is_san_diego]\n",
    "\n",
    "san_diego_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to slice by positional integer, you need to use `.iloc[]` (for **i**nteger **loc**action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_head = df.COUNTY_NAME.iloc[0:5]\n",
    "series_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise a pandas Series is very similar to a numpy array (often a numpy array is being used by pandas to store a Series!). You can do slicing, indexing, arithmetic, and boolean logic just like in numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas columns come with a range of helpful methods.\n",
    "\n",
    "There are methods for most of the same math equations as numpy: `min` `max`, `mean`  `mode` `median` `quantile`, `sum`, etc. Unlike numpy, pandas methods tend to ignore NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"WELL_DEPTH\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are methods for unique values. If a column is all the same, that could signify a data issue, or perhaps mean we we don't need to load that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"STN_ID\"].is_unique, \"There should be no duplicated IDs\"\n",
    "assert df[\"COUNTY_NAME\"].nunique() > 1,  \"Ensure we're not using a single-county subset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particularly useful method is `value_counts`. Used for summarizing discreet value distribution, it displays the count of each unique value.\n",
    "\n",
    "By default, `NaN` values aren't included, but for data exploration it's really important to know where our NaNs are so we add `dropna=False`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"WELL_USE\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column wrangling\n",
    "\n",
    "By passing a list of column names, we can slice all rows for only the specified columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"BASIN_NAME\", \"COUNTY_NAME\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New columns can be added as a singular value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"country_code\"] = \"US\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or with any list/Series/array of the same length as the DataFrame (pandas will convert it to a Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_code_prefix = []\n",
    "for code in df.SITE_CODE:\n",
    "    if code:\n",
    "        site_code_prefix.append(code.split(\"N\")[0])\n",
    "    else:\n",
    "        site_code_prefix.append(None)\n",
    "        \n",
    "df[\"site_code_prefix\"] = site_code_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by performing calculations between columns (`loc` is used for non-integer slicing, like by column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"well_depth_m\"] = df.WELL_DEPTH / 3.28084\n",
    "df[[\"site_code_prefix\", \"well_depth_m\"]].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single columns can be deleted with the `del` keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"ZIP_CODE\"]\n",
    "\n",
    "print(\"ZIP_CODE\" in df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for multiple columns, there's a drop method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"site_code_prefix\", \"well_depth_m\"])\n",
    "\n",
    "print(\"site_code_prefix\" in df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and slicing\n",
    "\n",
    "We've already seen the `head()` method, which shows the first n  rows.\n",
    "\n",
    "A similar function is the `sample()` method, which shows n *random* rows. This can give a better sense of the data, in case the first few rows aren't representative of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically what we're doing here isn't just printing some rows of our dataset, but actually creating a new DataFrame with some rows sliced from the old one, and printing that new frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other DataFrame methods that return a new dataframe with a subset of rows. `drop_duplicates()` returns a DataFrame with repeated rows removed. `dropna` returns a dataframe with only rows that don't have any NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates()\n",
    "df_clean = df_unique.dropna()\n",
    "len(df_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these are both DataFrame methods that return another DataFrame, we can **chain** them together to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop_duplicates().dropna()\n",
    "len(df_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most pandas methods return a new DataFrame rather than modifying the original one. We can see that our original still has the same number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.iloc` does integer indexing on a DataFrame too, but it's clearer to use the column names rather than their positional index. For that, `.loc` allows us to filter by both row and col at once (remember the row, col order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the USGS rows, and keep only the positional data.\n",
    "df_USGS = df.loc[df.WCR_NO == \"USGS\", [\"WCR_NO\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "df_USGS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.loc` is also used in pandas for assignment of a subset of rows/cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.COUNTY_NAME == \"Monterey\", \"WELL_USE\"] = \"Residential\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas index\n",
    "\n",
    "In the examples above, you might have noticed that when printing a DataFrame or a Series, it also shows a row number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[1000, 2000, 3000, 4000]][[\"STN_ID\", \"BASIN_NAME\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every DataFrame and Series has a row ID, which pandas calls an **index**.\n",
    "\n",
    "By default the index is the row number, but we could use the actual ID from our dataset for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = df.set_index(df.STN_ID.to_numpy())\n",
    "df_index.iloc[[1000, 2000, 3000, 4000]][[\"STN_ID\", \"BASIN_NAME\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes provide very fast lookups of individual rows using the `.loc` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index.loc[12002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's rare to access individual rows in analytical applications, so for simplicity we won't be using indexes in this course.\n",
    "\n",
    "You can always achieve the same result using boolean indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.STN_ID == 12002].iloc[0]  # Filter and take the first match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String series\n",
    "\n",
    "As well as numerical data, pandas Series class has methods for working with strings as well.\n",
    "\n",
    "We'll demo this with a dataset that has a few more strings: [CIWQS NPDES Permits](https://ciwqs.waterboards.ca.gov/ciwqs/readOnly/NpdesReportServlet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes = pd.read_excel(\"./data/npdes_data.xlsx\", nrows=1000, dtype={\"ZIP CODE\": str})\n",
    "df_npdes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to pull out all the permits related to AT&T.\n",
    "\n",
    "The problem is that there's inconsistent naming of the facilities (this is often the case with user-entered data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes.iloc[27:38][\"FACILIITY NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, lets tidy up the name field. We'll do this in a new column so we don't loose our original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes[\"tidy_name\"] = df_npdes[\"FACILIITY NAME\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of python's builtin string functions have equivalent pandas Series methods. The pandas methods are much faster though, and for advanced users, many can be used with [regular expressions](https://en.wikipedia.org/wiki/Regular_expression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace numeric NaN values with empty strings.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].fillna(\"\")\n",
    "\n",
    "# Remove leading/trailing whitespace.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].str.strip()\n",
    "\n",
    "# Convert to uppercase.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].str.upper()  \n",
    "\n",
    "# Fix spacing.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].str.replace(\"AT & T\", \"AT&T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new `tidy_name` column can now be used for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_att = df_npdes[df_npdes[\"tidy_name\"].str.contains(\"AT&T\")]\n",
    "\n",
    "df_att[[\"FACILIITY NAME\", \"tidy_name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also normalize to 5-digit zip codes by splitting on the dash, then taking the first group using the `str[]` indexing tool pandas provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes['tidy_zip_code'] = df_npdes[\"ZIP CODE\"].str.split('-').str[0]\n",
    "\n",
    "df_npdes[[\"ZIP CODE\", \"tidy_zip_code\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas documentation has a [Working with text data](https://pandas.pydata.org/docs/user_guide/text.html) guide that goes into more details about regular expressions as well as splitting/joining strings, and has a list of all the string methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime series\n",
    "\n",
    "Just like pandas groups string functions with a `.str` prefix, there is also a `.dt` prefix that contains functions for working with dates, times, and datetimes (timestamps).\n",
    "\n",
    "Let's have a look at some of our date columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\"ADOPTION DATE\", \"EFFECTIVE DATE\", \"EXPIRATION DATE\"]\n",
    "df_npdes[date_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes[date_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `object` dytpe indicates our dates weren't parsed correctly. There's also a numeric NaN mixed in there.\n",
    "\n",
    "To fix this we're going to have to go back to the data loading. In this case it's enough to tell python which columns to treat as dates with the `parse_dates` argument.\n",
    "\n",
    "(For more complex cases, you can specify a `date_format` argument, or use the `pd.to_datetime` function).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes = pd.read_excel(\"./data/npdes_data.xlsx\", nrows=1000, parse_dates=date_cols) \n",
    "df_npdes[date_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes[date_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dates have the correct type, we can use pandas date/time functionality! Missing/invalid timestamps are specified with `NaT` (not a timestamp).\n",
    "\n",
    "The `.dt` prefix has functions for accessing different parts of the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English day of week name. Then replace any None or NaNs with an empty string.\n",
    "df_npdes[\"ADOPTION DATE\"].dt.day_name().fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as functions for manipulating timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to the nearest hour (looks like our data is already rounded!).\n",
    "df_npdes[\"ADOPTION DATE\"].dt.round(\"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to timestamps, pandas also has the concept of differences between two timestamps.\n",
    "\n",
    "A `Timedelta` is a fixed difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift dates 7 days forward into the future).\n",
    "df_npdes[\"ADOPTION DATE\"] + pd.Timedelta(days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while an `offset` can vary on length depending on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 working days later.\n",
    "df_npdes[\"ADOPTION DATE\"] + pd.offsets.BusinessDay(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series mapping\n",
    "\n",
    "Series has a `map()` method that applies a regular python function separately to every element.\n",
    "\n",
    "This is helpful if your function is too complicated to represent using numpy/pandas arithmetic/methods, or if you'd like to use an existing function that doesn't support arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_zip_code(code):\n",
    "    \"\"\"Normalize and validate zip codes.\"\"\"\n",
    "    # Normalize to string.\n",
    "    if code is None or pd.isna(code):\n",
    "        return \"\"\n",
    "    code = str(code)\n",
    "\n",
    "    # Normalize to 5 digits.\n",
    "    code = code.split(\"-\")[0]\n",
    "    if len(code) == 4:\n",
    "        code = \"0\" + code\n",
    "\n",
    "    # Validate.\n",
    "    assert len(code) == 5, f\"Wrong zip length {code=}\"\n",
    "    return code\n",
    "\n",
    "\n",
    "df_npdes[\"validated_zip\"] = df_npdes[\"ZIP CODE\"].map(tidy_zip_code)\n",
    "\n",
    "df_npdes[[\"ZIP CODE\", \"validated_zip\"]].tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `map()` method also accepts a dictionary for more simple cases of input -> output mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_descriptions = {\n",
    "    \"INDSTW\": \"Industrial Stormwater\",\n",
    "    \"NPDMINING\": \"National Pollutant Discharge  - Mining\",\n",
    "    \"NPDNONMUNIPRCS\": \"National Pollutant Discharge  - Non-Municipal - Permit-required Confined Space\",\n",
    "    \"NPDMUNILRG\": \"National Pollutant Discharge  - Large Municipal\",\n",
    "}\n",
    "df_npdes[\"full_program_name\"] = df_npdes.PROGRAM.map(program_descriptions)\n",
    "\n",
    "df_npdes[[\"DISCHARGER\", \"PROGRAM\", \"full_program_name\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames\n",
    "\n",
    "To stack multiple DataFrames that have the same columns, pandas has the `concat` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_placer = pd.read_csv(\"data/gwl/Placer.csv\")\n",
    "df_sacramento = pd.read_csv(\"data/gwl/Sacramento.csv\")\n",
    "\n",
    "df_all = pd.concat([df_placer, df_sacramento])\n",
    "\n",
    "print(f\"{df_placer.shape=}\")\n",
    "print(f\"{df_sacramento.shape=}\")\n",
    "print(f\"{df_all.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex database-style joins, we have the `pandas.merge` function.\n",
    "\n",
    "Joins combine DataFrames by linking *rows* based on *matching column values*.\n",
    "\n",
    "Lets start by making a DataFrame from a dict of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program = pd.DataFrame({\n",
    "    \"PROGRAM\": [\"INDSTW\", \"NPDMINING\", \"NPDNONMUNIPRCS\",\"NPDMUNILRG\"],\n",
    "    \"program_name\": [\"Industrial Stormwater\", \"National Pollutant Discharge  - Mining\", \"National Pollutant Discharge  - Non-Municipal - Permit-required Confined Space\", \"National Pollutant Discharge  - Large Municipal\"],\n",
    "    \"program_launch_date\": pd.to_datetime([\"2024-01-01\", \"2023-12-20\", \"2016-01-20\", \"2024-01-01\"]),\n",
    "})\n",
    "\n",
    "df_program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and perform a join on the \"PROGRAM\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple inner join.\n",
    "df_joined = pd.merge(df_npdes, df_program, on=\"PROGRAM\")\n",
    "\n",
    "df_joined[[\"DISCHARGER\", \"PROGRAM\", \"program_name\", \"program_launch_date\"]].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our joined DataFrame has all the collumns from both input DataFrames!\n",
    "\n",
    "You might have noticed that the while `df_npdes` started off with 1000 rows, the last row number on `df_joined` is 818!\n",
    "\n",
    "By default, `merge()` does an **inner** merge, which means any rows with a value for `PROGRAM` that isn't in both DataFrames is discarded.\n",
    "\n",
    "Sometimes we do only care about the intersection of two datasets, in which case an inner merge is perfect! But more often in analysis we want to keep our main DataFrame intact, and just enhance with more columns wherever we have data. This is a **left** join.\n",
    "\n",
    "There's a couple more tweaks we can make to our join for improved resilience. \n",
    "\n",
    "* Often the joining column will have different names between our dataset. We can specify this with the `left_on` and `right_on` arguments. \n",
    "* If our incoming DataFrame has multiple rows with the same join column value, we can end up with extra data post-join. Passing `validate=\"many_to_one\"` enforces the lookup style of join.\n",
    "\n",
    "Here's a more typical left join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical data science left join.\n",
    "df_joined = pd.merge(\n",
    "    df_npdes,  # Left dataframe\n",
    "    df_program,  # Right dataframe,\n",
    "    how=\"left\",\n",
    "    left_on=\"PROGRAM\",\n",
    "    right_on=\"PROGRAM\",\n",
    "    validate=\"many_to_one\",\n",
    ")\n",
    "\n",
    "    \n",
    "df_joined[[\"DISCHARGER\", \"PROGRAM\", \"program_name\", \"program_launch_date\"]].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all 1000 rows are there, and we can clearly identify the added columns.\n",
    "\n",
    "The pandas documentation has a [join guide](https://pandas.pydata.org/docs/user_guide/merging.html#merge) that goes into more detail about the different join types, validation options, and merge arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "Grouping is another operation that will be familiar to those who have worked with databases.\n",
    "\n",
    "The idea is to categorise rows of a DataFrame into groups, then calculate summary statistics for each group.\n",
    "\n",
    "You first create a group by passing one or more columns to `groupby()`. Then define each aggregate with three pieces of information\n",
    "\n",
    "* the name of the result column is the name of the input argument\n",
    "* the first value of the tuple is the group's column to use\n",
    "* the second value of the tuple is the function to apply to the group. It can be the name of a pandas function as a string, or an actual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_frac(group):\n",
    "    \"\"\"The fraction of the group that has the value 'Active'\"\"\"\n",
    "    return np.mean(group == \"Active\")\n",
    "\n",
    "\n",
    "df_npdes.groupby([\"DISCHARGER STATE\"]).agg(\n",
    "    n_permits=(\"DISCHARGER ID\", \"size\"),\n",
    "    first_adoption=(\"ADOPTION DATE\", np.min),\n",
    "    active_frac=(\"RM STATUS\", active_frac),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with mathplotlib\n",
    "\n",
    "Because the module name is extremely long, it's customary to import using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we'll load in this DataFrame of groundwater measurements at a 10 different stations over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10 = pd.read_csv(\n",
    "    \"data/gwl/10-sites.csv\",\n",
    "    usecols=[\"SITE_CODE\", \"MSMT_DATE\", \"GSE_WSE\"],\n",
    "    parse_dates=[\"MSMT_DATE\"]\n",
    ")\n",
    "assert df_10.SITE_CODE.nunique() == 10\n",
    "\n",
    "df_10.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an empty plot using the `subplots()` function. This returns two things\n",
    "\n",
    "* a Figure object, which has methods for adjusting the plot layout and saving your plot\n",
    "* one or more Axes objects, which have methods for plotting data\n",
    "\n",
    "To plot a simple line for a single site, we need the `Axes.plot()` method, and pass first an array of our x-axis data, then our y-axis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site = df_10[df_10.SITE_CODE == \"384121N1212102W001\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_site.MSMT_DATE, df_site.GSE_WSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a basic plot but it works!\n",
    "\n",
    "Let's improve the readability of this plot a bit. Can you see in the code below where each of these changes was made?\n",
    "\n",
    "* Increase the resolution of the image (the default 100 dpi)\n",
    "* Label each axis\n",
    "* Label the entire figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.plot(df_site.MSMT_DATE, df_site.GSE_WSE)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Groundwater depth [ft]\")\n",
    "fig.suptitle(\"Site 384121N1212102W001 (Sacremento)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the same data using a scatter plot, with 50% transparent markers to show the density of overlapping points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.scatter(\n",
    "    df_site.MSMT_DATE,\n",
    "    df_site.GSE_WSE,\n",
    "    alpha=0.5,\n",
    "    color=\"black\",\n",
    "    s=5,  # Marker size.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple things can be plotted on the same Axes object. For example, it can be helpful to have both the interpolated line from `plot()` as well as the observation locations from `scatter()`. The most recent plot will be on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.scatter(\n",
    "    df_site.MSMT_DATE,\n",
    "    df_site.GSE_WSE,\n",
    "    alpha=0.2,\n",
    "    color=\"black\",\n",
    "    s=6,\n",
    ")\n",
    "ax.plot(df_site.MSMT_DATE, df_site.GSE_WSE, color=\"red\", alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our original dataset of 10 different stations, lets plot all 10 in a loop onto the same Axes object. To tell the lines apart, we add a `label=` argument to each `plot` call, then add a legend box to the axis with the `legend()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "\n",
    "for site_code in df_10.SITE_CODE.unique():\n",
    "    df_site_code = df_10[df_10.SITE_CODE == site_code]\n",
    "    ax.plot(df_site_code.MSMT_DATE, df_site_code.GSE_WSE, alpha=0.75, label=site_code)\n",
    "ax.legend(\n",
    "    fontsize=\"small\",\n",
    "    bbox_to_anchor=(1, 1),  # Shift legend outside of plot.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving plots\n",
    "\n",
    "You can save a plot using the Figure's `savefig` method. For instance, to save a pdf version of the most recent plot\n",
    "\n",
    "```python\n",
    "fig.savefig(\"10-site-water-depth.pdf\")\n",
    "```\n",
    "\n",
    "Even quicker in a noteook environment: right click on the plot, and copy/save as a PNG!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with seaborn\n",
    "\n",
    "With data that requires aggregation to plot, the seaborn package can handle much of the complexity for you.\n",
    "\n",
    "Seaborn is normally imported with the `sns` abbreviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using seaborn with DataFrames is a little different than with matplotlib directly. You typically pass the whole DataFrame as a `data` argument, then the column names (rather than the data) as the `x` and `y` arguments.\n",
    "\n",
    "Seaborn has it's own scatter function (`scatterplot`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "sns.scatterplot(\n",
    "    ax=ax,\n",
    "    data=df_site,\n",
    "    x=\"MSMT_DATE\",\n",
    "    y=\"GSE_WSE\",\n",
    "    alpha=0.5,\n",
    "    color=\"black\",\n",
    "    s=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With seaborn, we still use a matplotlib Figure and Axes. But we automatically get axis labels!\n",
    "\n",
    "Most seaborn functions perform some data processing before plotting. For example, `histplot` calculates and plots a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "sns.histplot(ax=ax, data=df_site, x=\"GSE_WSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The thin lines of a KDE distribution plot (`kdeplot()`) make it easier to compare in the distribution of all 10 sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "\n",
    "sns.kdeplot(ax=ax, data=df_10, x=\"GSE_WSE\", hue=\"SITE_CODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It's still a little messy though, a box plot might be clearer still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "sns.boxplot(ax=ax, data=df_10, x=\"GSE_WSE\", y=\"SITE_CODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn has dozens of plot functions, mostly invoking comparing distributions of data.\n",
    "\n",
    "The [seaborn gallery](https://seaborn.pydata.org/examples/index.html) is the place to go for a good overview of what seaborn can do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py4wds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
