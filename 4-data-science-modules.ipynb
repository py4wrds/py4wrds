{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Python data science modules\n",
    "\n",
    "Now we've got a grasp on the fundamentals of Python, we're ready to use Python for data science.\n",
    "\n",
    "Unlike some more math-oriented programming languages (like R or MATLAB), Python relies on external packages to provide most data science functionality.\n",
    "\n",
    "The Python community has largely standardized on the following packages, all of which we'll be covering in this module, and will be using extensively for the rest of the course:\n",
    "\n",
    "* pandas - working with tabular data\n",
    "* numpy - math tools, plus working with 1D & 2D arrays\n",
    "* matplotlib - low-level plotting\n",
    "* seaborn - high-level plotting \n",
    "\n",
    "\n",
    "## Pandas\n",
    "\n",
    "Pandas is a package for working with tabular data.\n",
    "\n",
    "Tabular data is anything in a table form! \n",
    "\n",
    "Common analytical examples include spreadsheets, CSV files, and database tables.\n",
    "\n",
    "Tabular data consists of rows and columns:\n",
    "\n",
    "* Each row represents an item, and each column represents a common feature of all the items.\n",
    "* Each row has the same columns as the other rows, in the same order.\n",
    "* A single column holds data of the same type, but different columns can have different types. \n",
    "* The order of rows sometimes matters, while the order of columns doesn't matter.\n",
    "\n",
    "\n",
    "Tabular data isn't just work spreadsheets either: for example, a music playlist is tabular data (for each song, we know the title, genre, etc)\n",
    "\n",
    "![Music playlist UI](img/4-itunes-ui.png)\n",
    "\n",
    "and your text message inbox is tabular data (for each conversation, we know the participants, unread status, data of most recent message, etc) \n",
    "\n",
    "\n",
    "![Messages UI](img/4-messages-ui.png)\n",
    "\n",
    "\n",
    "### Loading data\n",
    "\n",
    "\n",
    "Pandas comes with many functions for reading lots of different kinds of data. We'll cover all of these throughout the course, but here's a list of the main ones for your later reference\n",
    "\n",
    "* CSVs ([read_csv docs](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html), [py4wrds example](module-4-read-csv-example))\n",
    "* SQL databases\n",
    "* Excel files\n",
    "* Google sheets\n",
    "* Parquet files\n",
    "* Any of the above, as a URL\n",
    "\n",
    "\n",
    "\n",
    "To start off, we'll load this CSV file of ground water stations. \n",
    "\n",
    "<!-- TODO: after module deployment, replace r4wrds url with python one -->\n",
    "<!-- TODO: add screenshot to get raw csv -->\n",
    "(module-4-read-csv-example)=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# url = \"https://github.com/r4wrds/r4wrds/blob/main/intro/data/gwl/stations.csv?raw=true\"\n",
    "url = \"data/gwl/stations.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're loading the data directly from GitHub! This won't work for loading private files, you'll have to download the file and use the path instead of the url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame inspection\n",
    "\n",
    "The code above (`df = pd.read_csv(url)`) has loaded our tabular data into an object called a DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame is one of the two core classes that pandas gives us (the other is Series which represents a column).\n",
    "\n",
    "DataFrame has a number of attributes (variables) and methods (functions) for **inspecting** our data, which is the first step in any analysis!\n",
    "\n",
    "`head()` shows us the first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've got lots of columns, `.columns` shows all the names on one screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data are we working with? `.shape` gives us both the row and column count, which is always in row, col order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it's more clear to take the length directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_rows: {}\".format(len(df)))\n",
    "print(\"n_cols: {}\".format(len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dtypes` shows the data type of each column.\n",
    "\n",
    "This is important to check! Pandas makes some guesses about what data type to use, and often gets things wrong. Common pitfalls to be wary of include\n",
    "\n",
    "* Dates might be loaded as strings instead of rich datetime objects.\n",
    "* Numerical columns like `$145` or `78%` might be loaded as strings instead of as numbers\n",
    "* A single row with a typo (`32.111!`) or a non-numeric placeholder (using `Unknown` instead of `NaN`) will turn an other-wise numeric column into a string type.\n",
    "* Pandas defaults to using 64 bit integers and floats. If your dataset is maxing out your memory, you can specify 32 bit (or smaller) dtypes to reduce the size once loaded.\n",
    "* ZIP codes should be parsed as strings not integers, to avoid stripping ZIPs that begin with zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see pandas has done a pretty good job here! (the `object` type is what pandas uses to represent strings).\n",
    "\n",
    "Fixing some dtype issues can involve more complex analysis. But for simple cases, we can simply tell pandas what to do when loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, dtype={\"ZIP_CODE\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`describe()` gives a summary of our numerical columns. With the reloaded dataframe, `ZIP_CODE` is no longer considered numeric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're not always interested in the whole dataset for every analysis. You can load a subset of the columns to speed up loading, reduce memory pressure, and to just keep your workspace tidier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_county = pd.read_csv(url, usecols=[\"STN_ID\", \"WELL_DEPTH\"])\n",
    "\n",
    "df_county.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "\n",
    "A column of a dataframe is a Series object. You can access a column by using it's name in `[]` brackets, just like a dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"BASIN_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"COUNTY_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series is conceptually very similar to a Python list, except all the values in a Series are the same data type. We can convert a Series to a list with `to_list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"COUNTY_NAME\"].to_list()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas columns come with a huge range of statistical methods.\n",
    "\n",
    "\n",
    "There are methods for descriptive statistics like `min` `max`, `mean`  `mode` `median` `quantile`, `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"WELL_DEPTH\"].quantile(0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are methods for  unique values. If a column is all the same, that could signify a data issue, or perhaps mean we we don't need to load that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"STN_ID\"].is_unique, \"There should be no duplicated IDs\"\n",
    "assert df[\"COUNTY_NAME\"].nunique() > 1,  \"Ensure we're not using a single-county subset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `value_counts` method is great for summarizing non-numerical columns, showing the count of each unique value.\n",
    "\n",
    "By default, `NaN` values aren't included, but for data exploration it's really important to know where our NaNs are so we add `dropna=False`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"WELL_USE\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas' method library is large and growing larger, no-one can keep track of all these methods! It's common in pandas development to be frequently asking Google / ChatGPT \"how to round numbers in a pandas series\". \n",
    "\n",
    "The pandas [official documentation](https://pandas.pydata.org/docs/reference/series.html) is another great resource: each method has multiple examples and detailed descriptions of the parameters and statistical algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LATITUDE\"].round(decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as methods, Series also support arithmetic (unlike a Python list).\n",
    "\n",
    "For example, we can convert well depth to meters and save it as a new column on our data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"WELL_DEPTH_METERS\"] = df[\"WELL_DEPTH\"] * 0.3048\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and slicing\n",
    "\n",
    "We've already seen the `head()` method, which shows the first n (5 by default) rows.\n",
    "\n",
    "A similar function is the `sample()` method, which shows n random rows. This can give a better sense of the data, in case the first few rows aren't representative of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically what we're doing here isn't just printing some rows of our dataset, but actually creating a new DataFrame with some rows sliced from the old one, and printing that new frame. `iloc` is similar to `head` but lets you specify the row range to slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dozen_rows = df.iloc[10:24]\n",
    "len(df_dozen_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other DataFrame methods that return a new dataframe with a subset of rows. `drop_duplicates()` returns a DataFrame with repeated rows removed. `dropna` returns a dataframe with only rows that don't have any NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates()\n",
    "df_clean = df_unique.dropna()\n",
    "len(df_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these are both DataFrame methods that return another DataFrame, we can **chain** them together to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop_duplicates().dropna()\n",
    "len(df_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most pandas methods return a new DataFrame rather than modifying the original one. We can see that our original still has the same number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own filters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USGS = df[df.WCR_NO == \"USGS\"]\n",
    "df_USGS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing by column\n",
    "\n",
    "By passing a list of column names, we can slice all rows for only the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"BASIN_NAME\", \"COUNTY_NAME\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also delete columns using the `del` keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"ZIP_CODE\"]\n",
    "\n",
    "print(\"ZIP_CODE\" in df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String columns\n",
    "\n",
    "As well as numerical data, pandas Series class has methods for working with strings as well.\n",
    "\n",
    "We'll demo this with a dataset that has a few more strings: [CIWQS NPDES Permits](https://ciwqs.waterboards.ca.gov/ciwqs/readOnly/NpdesReportServlet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes = pd.read_excel(\"./data/npdes_data.xlsx\", nrows=1000, dtype={\"ZIP CODE\": str})\n",
    "df_npdes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to pull out all the permits related to AT&T.\n",
    "\n",
    "The problem is that there's inconsistent naming of the facilities (this is often the case with user-entered data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes.iloc[27:38][\"FACILIITY NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, lets tidy up the name field. We'll do this in a new column so we don't loose our original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes[\"tidy_name\"] = df_npdes[\"FACILIITY NAME\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of python's builtin string functions have equivalent pandas Series methods. The pandas methods are much faster though, and for advanced users, many can be used with [regular expressions](https://en.wikipedia.org/wiki/Regular_expression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace numeric NaN values with empty strings.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].fillna(\"\")\n",
    "\n",
    "# Remove leading/trailing whitespace.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].str.strip()\n",
    "\n",
    "# Convert to uppercase.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].str.upper()  \n",
    "\n",
    "# Fix spacing.\n",
    "df_npdes[\"tidy_name\"] = df_npdes[\"tidy_name\"].str.replace(\"AT & T\", \"AT&T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new `tidy_name` column can now be used for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_att = df_npdes[df_npdes[\"tidy_name\"].str.contains(\"AT&T\")]\n",
    "\n",
    "df_att[[\"FACILIITY NAME\", \"tidy_name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also normalize to 5-digit zip codes by splitting on the dash, then taking the first group using the `str[]` indexing tool pandas provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes['tidy_zip_code'] = df_npdes[\"ZIP CODE\"].str.split('-').str[0]\n",
    "\n",
    "df_npdes[[\"ZIP CODE\", \"tidy_zip_code\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas documentation has a [Working with text data](https://pandas.pydata.org/docs/user_guide/text.html) guide that goes into more details about regular expressions as well as splitting/joining strings, and has a list of all the string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime columns\n",
    "\n",
    "Just like pandas groups string functions with a `.str` prefix, there is also a `.dt` prefix that contains functions for working with dates, times, and datetimes (timestamps).\n",
    "\n",
    "Let's have a look at some of our date columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\"ADOPTION DATE\", \"EFFECTIVE DATE\", \"EXPIRATION DATE\"]\n",
    "df_npdes[date_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes[date_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dates were just loaded as strings: we also have a numeric `NaN` mixed in there.\n",
    "\n",
    "To fix this we're going to have to go back to the data loading. In this case it's enough to tell python which columns to treat as dates with the `parse_dates` argument.\n",
    "\n",
    "(For more complex cases, you can specify a `date_format` argument, or use the `pd.to_datetime` function).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes = pd.read_excel(\"./data/npdes_data.xlsx\", nrows=1000, parse_dates=date_cols) \n",
    "df_npdes[date_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npdes[date_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dates have the correct type, we can use pandas date/time functionality!\n",
    "\n",
    "The `.dt` prefix has functions for accessing different parts of the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English day of week name. Then replace any None or NaNs with an empty string.\n",
    "df_npdes[\"ADOPTION DATE\"].dt.day_name().fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as functions for manipulating timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to the nearest hour (looks like our data is already rounded!).\n",
    "df_npdes[\"ADOPTION DATE\"].dt.round(\"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to timestamps, pandas also has the concept of differences between two timestamps.\n",
    "\n",
    "A `Timedelta` is a fixed difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift dates 7 days forward into the future).\n",
    "df_npdes[\"ADOPTION DATE\"] + pd.Timedelta(days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while an `offset` can vary on length depending on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 working days later.\n",
    "df_npdes[\"ADOPTION DATE\"] + pd.offsets.BusinessDay(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_npdes[\"ADOPTION DATE\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_npdes[\"ADOPTION DATE\"].to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(df_npdes[\"ADOPTION DATE\"].to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_npdes[\"adoption_data_tidy\"] = pd.to_datetime(df_npdes[\"ADOPTION DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Numpy\n",
    "\n",
    "Numpy is a Python package for representing **array data**, and comes with a large library of tools and mathematical functions that operate efficiently on arrays.\n",
    "\n",
    "If you're familiar with more math-oriented programming languages like R or MATLAB, numpy brings much of the builtin math and data functionality from those languages into Python.\n",
    "\n",
    "Numpy is by far the most popular Python package for data science, and is one of the [most-downloaded](https://pypistats.org/top) python packages overall. It's so useful and reliable, that most of the mathematical functionality of the other packages covered in this module (pandas, seaborn, matplotlib) is provided by numpy under the hood.\n",
    "\n",
    "Because numpy is used a lot, it's convention to import it with the `np` abbreviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Why numpy?\n",
    "\n",
    "A numpy array is similar to a Python list: they can both serve as containers for numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_list = [0, 2, 4, 6]\n",
    "print(python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = np.array([0, 2, 4, 6])\n",
    "print(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why use numpy instead of lists?\n",
    "\n",
    "* Speed\n",
    "    * Although numpy is a Python package, most of the functionality is written in fast C or Fortran code.\n",
    "* Memory efficient\n",
    "    * Numpy uses less memory to store numbers than Python, so you can work on larger datasets.\n",
    "* Functionality\n",
    "    * Numpy comes with a huge range of modules with fast and thoroughly-validated algorithms from interpolation to fourier transforms.\n",
    "* Manipulation syntax\n",
    "    * Numpy's syntax makes it clear and easy to perform common array operations, like slicing, filtering, and summarization.\n",
    "\n",
    "\n",
    "But there are some usecases where lists make more sense\n",
    "\n",
    "* Storing different kinds of data together\n",
    "    * Numpy arrays are homogeneous, all the elements must be the same type\n",
    "* Working with non-numerical data\n",
    "    * Some numpy functionality works with strings and other types, but performance can suffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Array slicing and indexing\n",
    "\n",
    "\n",
    "One way to create an array is from a Python sequence like a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month = np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31])\n",
    "days_per_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the original Python list, arrays can be sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and individual elements can be index out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(days_per_month[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-dimensional (and higher dimensional, there's no limit in numpy!) arrays can be created from nested Python sequences:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "array_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You index a 2D array using the same notation: first your row slicing, then a comma `,`, then your column slicing.\n",
    "\n",
    "For example the first element of the second row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_2d[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the top three values of the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_2d[0:3, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how our column has lost its \"verticalness\": once we've sliced it out, it's just a regular 1D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating arrays\n",
    "\n",
    "As well as converting Python lists to arrays, numpy can create its own arrays!\n",
    "\n",
    "You can create an array that's filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or ones (remember numpy uses the row, col ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has it's own version of the Python `range` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(2, 9, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a related `linspace` function to create an array with elements evenly spaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0, 10, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array attributes\n",
    "\n",
    "The shape attribute gives the rows and cols (in that order!) of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Standard library  \n",
    "2. Numpy  \n",
    "   1. Why numpy?   \n",
    "   2. Creating Arrays  \n",
    "   3. Array Dimensions  \n",
    "   4. Array Operations   \n",
    "   5. Slicing, Indexing, and Broadcasting  \n",
    "   6. Dot product, cross product, matrix multiplication  \n",
    "   7. Exporting and loading arrays   \n",
    "3. Pandas  \n",
    "   1. Dataframes  \n",
    "   2. DataFrame structure  \n",
    "      1. Columns  \n",
    "      2. Index, datetime index, datetime module   \n",
    "   3. Loading dataframes from .csv and .xls files   \n",
    "      1. Dealing with messy data  \n",
    "      2. Example dataset  \n",
    "      3. Cleaning real messy data  \n",
    "   4. Selecting columns  \n",
    "   5. Filtering by conditionals  \n",
    "   6. Helpful dataframe functions   \n",
    "      1. Convert a dict to a dataframe   \n",
    "   7. Advanced dataframe topics  \n",
    "      1. Multiindex   \n",
    "      2. .apply   \n",
    "      3. .groupby   \n",
    "4. Matplotlib  \n",
    "   1. Line plot  \n",
    "   2. Scatterplot  \n",
    "   3. Plotting 2d arrays with imshow  \n",
    "   4. Formatting plots   \n",
    "      1. Title  \n",
    "      2. Axis labels  \n",
    "      3. Legend  \n",
    "5. Seaborn  \n",
    "   1. Relplot  \n",
    "   2. Distplot   \n",
    "   3. Catplot   \n",
    "6. Practical Example \\- scatterplot and linear regression   \n",
    "   1. Load two datasets  \n",
    "   2. Create pandas dataframe with each as a column  \n",
    "   3. Do a linear regression between two columns  \n",
    "   4. Plot scatterplot and linear regression using matplotlib  \n",
    "   5. Add axis labels, legend, title, regression equation   \n",
    "   6. Save plot to .png \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
